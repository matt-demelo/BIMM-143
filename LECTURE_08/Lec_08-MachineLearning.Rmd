---
title: "Lecture_08 - Intro to Machine Learning"
author: "Matt Demelo"
date: "4/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering

Let's start off with an example run of the **kmeans()** function:

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3)) # 30 points centered on -3, 30 points center on +3
x <- cbind(x=tmp, y=rev(tmp)) # binds the two together as a matrix

plot(x)
```


Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster?
  *30 in each*
Q. What ‘component’ of your result object details
 - cluster size?
  *"size"* -- call x_clus$size
 - cluster assignment/membership?
  *"cluster"* call x_clus$cluster
 - cluster center?
  *"centers"* call x_clus$centers
Plot x colored by the kmeans cluster assignment and
 add cluster centers as blue points
 
```{r}
x_clus <- kmeans(x, centers = 2, nstart = 20)
x_clus
```
 
```{r}
x_clus$cluster
```
 
 
```{r}
plot(x, col = x_clus$cluster)
points(x_clus$centers, pch = 18, col = "blue", cex = 3)
```
 
## Hierarchical Clustering Example


Must give the **hclust()** function a distance matrix, not raw data, as an input.
```{r}

# Distance matrix calc.
d <- dist(x)

# Clustering
hc <- hclust(d)
plot(hc)

```

This dendrogram makes sense: it starts from individual points, and begins clustering from there. It also shows us the structure of the data: which points lie in which groups, and so on. It is easy to visualzie that the points converge into two main groups

```{r}
plot(hc)
abline(h = 6, col = 2) # adds a height cutoff

```

```{r}

cutree(hc, h = 6) # groups clusters that fall below a height cut off, and returns a vector of clusters

```


```{r}
cutree(hc, k = 2) # groups data into k# of clusters, and gives a vector output as well
```

### Now with a more complicated dataset, with significant overlap.

```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")

```

```{r}
# Step 2. Plot the data without clustering
plot(x)
```

```{r}
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)

col <- as.factor( rep(c("c1","c2","c3"), each=50) )

plot(x, col=col)
```

Q. Use the dist(), hclust(), plot() and cutree()
 functions to return 2 and 3 clusters
Q. How does this compare to your known 'col' groups?

#### Now we use hclust to cluster the data hierarchically

```{r}
# *STEP 1*

# Converting x to distance data
xdist <- dist(x)

# Clustering of xdist, hierarchical
hier_x <- hclust(xdist)
hier_x

```

```{r}
# *STEP 2*

plot(hier_x, main = "2 Clusters")
abline(h = 2.5, col = 2)

plot(hier_x, main = "3 Clusters")
abline(h=1.75, col = 3)
```

```{r}
# *STEP 3.1*
gp2 <- cutree(hier_x, k = 2) # 2 clusters
gp2

```

```{r}
# *STEP 3.2*
gp3 <- cutree(hier_x, k = 3) # 2 clusters
gp3

```

```{r}
plot(x, col = gp2)
```

The two-way clustering seems to be insufficient for clustering the data, so how about a three-way cluster?

```{r}
plot(x, col = gp3)
```

The three way clustering isn't perfect, but it's a lot better than it was before we used hclust().

Let's look at how many groups we have in each cluster:

```{r}
table(gp2)
```

```{r}
table(gp3)
```

What about a cross comparison?

```{r}
table(gp2, gp3)
```

The results seem confusing at first, but in comparing them to our earlier tables, we can see that what gp3 did was split gp2's cluster 1 (n = 89) into two separate clusters, while retaining cluster 2 as cluster 3. If we look at our dendograms, we can easily see where the clustering would occur


# PCA: Principal Component Analysis

We will use **prcomp()** function for PCA

```{r}
## You can also download this file from the class website!
mydata <- read.csv("https://tinyurl.com/expression-CSV",
 row.names=1)
head(mydata, 10)
```

```{r}
# How many genes do we have?

nrow(mydata)
```

100 genes

```{r}
# How many organisms are we looking at?

ncol(mydata)
```

10 organisms.

prcomp() will expect the data to be samples to be in rows, and genes to be in columns, so we must transpose

```{r}
t(head(mydata))
```

```{r}
colnames(mydata)
```

Running the PCA on our data (transposed)

```{r}
pca <- prcomp(t(mydata), scale = T)
attributes(pca) # SO WE CAN LOOK AT WHAT IS IN OUR PCA
```

Now we can plot our PCA.

```{r}
plot(pca$x[,1], pca$x[,2], xlab = "PC1", ylab ="PC2") 

## Variance captured per PC
pca.var <-  pca$sdev^2

## Percent variance captured per PC
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
```

```{r}
head(pca.var.per)
```

Now we can see the percent variance captured by each PC. This gives us a visual representation of how effective each PC is.

Let's plot this

```{r}
barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")
```

Now we can make our PCA plot look all pretty, like something you'd see in a journal

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
 xlab=paste0("PC1 (", pca.var.per[1], "%)"),
 ylab=paste0("PC2 (", pca.var.per[2], "%)")) 
```


## Now to do some PCA for our UK Foods Dataset

Let's start by reading the data into a data frame:

```{r}
foods <- read.csv("UK_foods.csv", row.names = 1)
head(foods)
```

First we try looking at the data:

```{r}
barplot(as.matrix(foods), beside=F, col=rainbow(nrow(foods)))
```

Now we should try looking at pairwise plots of food data:
```{r}
pairs(foods, col=rainbow(10), pch=16)

```


Now we run the PCA, to make some sense of this

```{r}
tfoods <- t(foods)
foodpca <- prcomp(tfoods)
summary(foodpca)
```

Now we plot the PCA:

```{r}
plot(foodpca$x[,1], foodpca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(foodpca$x[,1], foodpca$x[,2], colnames(foods))
```

```{r}
colnames(foods)
```


```{r}
mycols <- c("red","orange","green","blue")
```


Now that we've done that, we can make our plot prettier

```{r}
plot(foodpca$x[,1], foodpca$x[,2],main = "PCA: UK Foods", xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(foodpca$x[,1], foodpca$x[,2], colnames(foods), col = mycols)
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( foodpca$rotation[,1], las=2 )
```










